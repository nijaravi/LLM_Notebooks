{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbB6tKPM9u7V"
   },
   "source": [
    "# Grounding Generative AI using Enterprise Search Results\n",
    "\n",
    "_This colab demonstrates examples of Retrieval Augmented Generation (RAG)\n",
    "and Reasoning + Acting (ReAct) workflows that make use of Google Enterprise Search (ES) and Large Language Models (LLMs). Langchain is also used for many Quality of Life features. Please make a separate copy of this notebook before making any changes._\n",
    "\n",
    "\n",
    "* _**Credit to and  inspiration** `rthallam@` for their great work on langchain integration [generative-ai Github repo](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/language/examples/langchain-intro)_\n",
    "\n",
    "* _**Author**: `saurabhmangal@`_\n",
    "* _**Last Update**: June 31th 2023_\n",
    "* _**Context**: [RAG Arxiv Paper](https://arxiv.org/pdf/2005.11401.pdf) & [ReACT Arxiv Paper](https://arxiv.org/abs/2210.03629.pdf)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JwTux4w_Lc_n"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# from google.auth import default\n",
    "\n",
    "# auth.authenticate_user()\n",
    "# creds, _ = default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hOvJZRHiQ6z"
   },
   "source": [
    "# Installation\n",
    "**NOTE: This requires shutting down the runtime.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "P3iDEOs_iR39",
    "outputId": "3633b279-1b78-437a-b173-4bcb1ac93a12"
   },
   "outputs": [],
   "source": [
    "# # Install langchain\n",
    "# ! pip install langchain --upgrade\n",
    "\n",
    "# # Install Enterprise Search SDK and Vertex PaLM endpoint\n",
    "# ! pip install google_cloud_discoveryengine\n",
    "\n",
    "# # Install Google Cloud Platform\n",
    "# ! pip install google-cloud-aiplatform --upgrade\n",
    "\n",
    "# import IPython\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_8xAnmQ958i"
   },
   "source": [
    "# Imports and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "pYwoBWBvjobh"
   },
   "outputs": [],
   "source": [
    "from google.cloud import discoveryengine_v1beta\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.callbacks.manager import Callbacks, CallbackManagerForChainRun\n",
    "from langchain.schema import AgentAction, AgentFinish, Document\n",
    "\n",
    "from typing import Any, Mapping, List, Dict, Optional, Tuple, Sequence, Union\n",
    "import json, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QKQ3_xIyPdvj",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Additional Enterprise Search Classes and helper functions\n",
    "\n",
    "class EnterpriseSearchRetriever():\n",
    "  \"\"\"Retriever class to fetch documents or snippets from a search engine.\"\"\"\n",
    "  def __init__(self,\n",
    "               project,\n",
    "               search_engine,\n",
    "               location='global',\n",
    "               serving_config_id='default_config'):\n",
    "    self.search_client = discoveryengine_v1beta.SearchServiceClient()\n",
    "    self.serving_config: str = self.search_client.serving_config_path(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            data_store=search_engine,\n",
    "            serving_config=serving_config_id,\n",
    "            )\n",
    "\n",
    "  def _search(self, query:str):\n",
    "    \"\"\"Helper function to run a search\"\"\"\n",
    "    request = discoveryengine_v1beta.SearchRequest(serving_config=self.serving_config, query=query)\n",
    "    return self.search_client.search(request)\n",
    "\n",
    "  def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "    \"\"\"Retrieve langchain Documents from a search response\"\"\"\n",
    "    res = self._search(query)\n",
    "    documents = []\n",
    "    for result in res.results:\n",
    "        data = MessageToDict(result.document._pb)\n",
    "        metadata = data.copy()\n",
    "        if 'derivedStructData' in metadata:\n",
    "            del metadata['derivedStructData']\n",
    "        if 'structData' in metadata:\n",
    "            del metadata['structData']\n",
    "        if data.get('derivedStructData') is None:\n",
    "            content = json.dumps(data.get('structData', {}))\n",
    "        else:\n",
    "            content = json.dumps([d.get('snippet') for d in data.get('derivedStructData', {}).get('snippets', []) if d.get('snippet') is not None])\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "    return documents\n",
    "\n",
    "  def get_relevant_snippets(self, query: str) -> List[str]:\n",
    "    \"\"\"Retrieve snippets from a search query\"\"\"\n",
    "    res = self._search(query)\n",
    "    snippets = []\n",
    "    for result in res.results:\n",
    "        data = MessageToDict(result.document._pb)\n",
    "        if data.get('derivedStructData', {}) == {}:\n",
    "            snippets.append(json.dumps(data.get('structData', {})))\n",
    "        else:\n",
    "            snippets.extend([d.get('snippet') for d in data.get('derivedStructData', {}).get('snippets', []) if d.get('snippet') is not None])\n",
    "    return snippets\n",
    "\n",
    "\n",
    "class EnterpriseSearchChain(Chain):\n",
    "    \"\"\"Chain that queries an Enterprise Search Engine and summarizes the responses.\"\"\"\n",
    "\n",
    "    chain: Optional[LLMChain]\n",
    "    search_client: Optional[EnterpriseSearchRetriever]\n",
    "\n",
    "    def __init__(self,\n",
    "                 project,\n",
    "                 search_engine,\n",
    "                 chain,\n",
    "                 location='global',\n",
    "                 serving_config_id='default_config'):\n",
    "        super().__init__()\n",
    "        self.chain = chain\n",
    "        self.search_client = EnterpriseSearchRetriever(project, search_engine, location, serving_config_id)\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return ['query']\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['summary']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
    "        _run_manager = CallbackManagerForChainRun.get_noop_manager()\n",
    "        query = inputs['query']\n",
    "        _run_manager.on_text(query, color=\"green\", end=\"\\n\", verbose=self.verbose)\n",
    "        snippets = self.search_client.get_relevant_snippets(query)\n",
    "        documents = self.search_client.get_relevant_documents(query) ## new line\n",
    "        # get_relevant_documents\n",
    "        _run_manager.on_text(snippets, color=\"white\", end=\"\\n\", verbose=self.verbose)\n",
    "        summary = self.chain.run(snippets)\n",
    "        return {'summary': summary, 'documents' : documents} ## modified line\n",
    "\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"google_enterprise_search_chain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "brkqRojUm-ad"
   },
   "outputs": [],
   "source": [
    "#@title ### You will need to update these values\n",
    "\n",
    "VERTEX_API_PROJECT = PROJECT_ID = \"my-project-0004-346516\" #'your-project' #@param {\"type\": \"string\"}\n",
    "VERTEX_API_LOCATION =REGION= 'us-central1' #@param {\"type\": \"string\"}\n",
    "\n",
    "import vertexai\n",
    "vertexai.init(project=VERTEX_API_PROJECT, location=VERTEX_API_LOCATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Vj6bJb5tScta"
   },
   "outputs": [],
   "source": [
    "#@title Initialise the LLM\n",
    "GCP_PROJECT = VERTEX_API_PROJECT\n",
    "SEARCH_ENGINE = 'customer-store_1695195086499'\n",
    "\n",
    "# \"customer-app_1695194973554\" #@param {type: \"string\"} test002_1686801972135/servingConfigs/default_search:search\n",
    "# LLM_MODEL = \"text-bison@001\" #@param {type: \"string\"}\n",
    "LLM_MODEL = \"text-bison@latest\" #@param {type: \"string\"}\n",
    "\n",
    "\n",
    "MAX_OUTPUT_TOKENS = 1024 #@param {type: \"integer\"}\n",
    "TEMPERATURE = 0.4 #@param {type: \"number\"}\n",
    "TOP_P = 0.8 #@param {type: \"number\"}\n",
    "TOP_K = 40 #@param {type: \"number\"}\n",
    "VERBOSE = True #@param {type: \"boolean\"}\n",
    "llm_params = dict(\n",
    "    model_name=LLM_MODEL,\n",
    "    max_output_tokens=MAX_OUTPUT_TOKENS,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=TOP_P,\n",
    "    top_k=TOP_K,\n",
    "    verbose=VERBOSE,\n",
    ")\n",
    "\n",
    "llm = VertexAI(**llm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what type of animals can be insured\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease parse these search results for the question: [] , in case of no results say nothing found\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Nothing found']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Example - summarize financial results\n",
    "SEARCH_QUERY = 'Does the policy cover hereditary or genetic conditions?' #@param {type: \"string\"}\n",
    "\n",
    "SEARCH_QUERY = 'what type of animals can be insured' #@param {type: \"string\"}\n",
    "\n",
    "PROMPT_STRING = \"Please parse these search results for the question: {results} , in case of no results say nothing found\" #@param {type: \"string\"}\n",
    "\n",
    "\n",
    "# '''\n",
    "# Strictly say that \"information not found in the policy document. For specific\n",
    "# details on this aspect, it is best to refer to the full policy\n",
    "# documentation or directly consult with the insurance provide\" if it is not found in the content above. First search the information in the context above and based on on the exact match try to answer the question. Also add reference location to where this information is found (in '[]' brackets)\n",
    "\n",
    "# Does the policy cover hereditary or genetic conditions?\n",
    "# '''\n",
    "\n",
    "# Combine the LLM with a prompt to make a simple chain\n",
    "prompt = PromptTemplate(input_variables=['results'],\n",
    "                        template=PROMPT_STRING)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "\n",
    "# Combine this chain with Enterprise Search in a new chain\n",
    "es_chain = EnterpriseSearchChain(project=GCP_PROJECT,\n",
    "                                 search_engine=SEARCH_ENGINE,\n",
    "                                 chain=chain)\n",
    "print(SEARCH_QUERY)\n",
    "result = es_chain.run(SEARCH_QUERY)\n",
    "\n",
    "result.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8i4hrvoC-IP",
    "outputId": "aea00f8c-b287-4ab2-9d00-7cf4ead4b317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease parse these search results of : [] \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ```',\n",
       " '{',\n",
       " '  \"took\": 1,',\n",
       " '  \"timed_out\": false,',\n",
       " '  \"_shards\": {',\n",
       " '    \"total\": 1,',\n",
       " '    \"successful\": 1,',\n",
       " '    \"skipped\": 0,',\n",
       " '    \"failed\": 0',\n",
       " '  },',\n",
       " '  \"hits\": {',\n",
       " '    \"total\": 0,',\n",
       " '    \"max_score\": null,',\n",
       " '    \"hits\": []',\n",
       " '  }',\n",
       " '}',\n",
       " '```']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Example - summarize financial results\n",
    "SEARCH_QUERY = 'what type of animals can be insured' #@param {type: \"string\"}\n",
    "PROMPT_STRING = \"Please parse these search results of : {results} \" #@param {type: \"string\"}\n",
    "\n",
    "# Combine the LLM with a prompt to make a simple chain\n",
    "prompt = PromptTemplate(input_variables=['results'],\n",
    "                        template=PROMPT_STRING)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "\n",
    "# Combine this chain with Enterprise Search in a new chain\n",
    "es_chain = EnterpriseSearchChain(project=GCP_PROJECT,\n",
    "                                 search_engine=SEARCH_ENGINE,\n",
    "                                 chain=chain)\n",
    "\n",
    "result = es_chain.run(SEARCH_QUERY)\n",
    "\n",
    "result.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xb5zRJsmDO-P",
    "outputId": "1c8dece7-5190-4e89-8305-253c08bcf944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please parse these search results and summarize them to answer\n",
      "the following question.\n",
      "Results:\n",
      "[]\n",
      "\n",
      "Question:'How many types of insurance available in  cats\n",
      "Answer:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' 0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Example - Answer a question about a search query\n",
    "SEARCH_QUERY = 'dogs' #@param {type: \"string\"}\n",
    "YEAR = ' cats' #@param {type:\"string\"}\n",
    "\n",
    "PROMPT_STRING = \"\"\"\n",
    "Please parse these search results and summarize them to answer\n",
    "the following question.\n",
    "Results:\n",
    "{results}\n",
    "\n",
    "Question:'How many types of insurance available in \"\"\" + YEAR + \"\"\"\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Combine the LLM with a prompt to make a simple chain\n",
    "prompt = PromptTemplate(input_variables=['results'],\n",
    "                        template=PROMPT_STRING)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "\n",
    "# Combine this chain with Enterprise Search in a new chain\n",
    "# This chain simply combines a Langchain LLMChain with our EnterpriseSearchRetriever\n",
    "es_chain = EnterpriseSearchChain(project=GCP_PROJECT,\n",
    "                                 search_engine=SEARCH_ENGINE,\n",
    "                                 chain=chain)\n",
    "\n",
    "result = es_chain.run(SEARCH_QUERY)\n",
    "\n",
    "result.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "_f_2_p-EgHZk",
    "outputId": "983d98ac-5155-4401-94b6-f879132d50a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mExtract the most specific search terms from the following query:\n",
      "\n",
      "Query:\n",
      "'policy terms, what are they?'\n",
      "\n",
      "Search Terms:\n",
      "* \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' policy \\n* terms'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@markdown ## Choose a search engine and define a complex query\n",
    "COMPLEX_QUERY = 'policy terms, what are they?' #@param {\"type\": \"string\"}\n",
    "\n",
    "# Initialise an Enterprise Search Retriever\n",
    "retriever = EnterpriseSearchRetriever(GCP_PROJECT, SEARCH_ENGINE)\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"complex_query\"], template=\"\"\"Extract the most specific search terms from the following query:\n",
    "\n",
    "Query:\n",
    "'{complex_query}'\n",
    "\n",
    "Search Terms:\n",
    "* \"\"\")\n",
    "\n",
    "#@markdown ## Fetch results from the LLM\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "terms = chain.run(COMPLEX_QUERY)\n",
    "\n",
    "#@markdown ```\n",
    "#@markdown * \"revenue\"\n",
    "#@markdown * \"budget\"\n",
    "#@markdown * \"travel\"\n",
    "#@markdown ```\n",
    "\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFSyioUag9wk",
    "outputId": "07a5324f-462e-4e90-9753-fad6c1694dad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['policy', 'terms']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Clean up the response\n",
    "#@markdown ## Select suitable terms and clean up unneccessary characters\n",
    "lines_to_ignore = 0 #@param {\"type\": \"integer\"}\n",
    "max_terms_to_search = 4 #@param {\"type\": \"integer\"}\n",
    "\n",
    "clean_terms = [re.sub('[^\\d\\w\\s]', '', q).strip() for q in terms.split('\\n')[lines_to_ignore:lines_to_ignore + max_terms_to_search]]\n",
    "\n",
    "#@markdown `['disclosure', 'appraisal', 'consultation', 'draft', 'sep']`\n",
    "clean_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M1mh4Ik6hf5h",
    "outputId": "b3347ef0-3932-4637-91d2-7df2811c790c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@markdown ## Search each term and keep the top `n` results\n",
    "num_results = 5 #@param {\"type\": \"integer\"}\n",
    "\n",
    "results = []\n",
    "for q in clean_terms:\n",
    "  snippets = retriever.get_relevant_snippets(q)\n",
    "  results.extend([s for s in snippets[:num_results]])\n",
    "\n",
    "results = list(set(results)) # Deduplicate to keep prompt length down\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IupS3S8zjY0h",
    "outputId": "e5b92ee0-df23-44e0-a293-78154202e938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease summarize the following contextual data to answer the following question. Provide references to the context in your answer:\n",
      "Question: policy terms, what are they?\n",
      "Context:\n",
      "[]\n",
      "Answer with citations:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' A policy term is a word or phrase used in an insurance policy that defines the coverage provided. ',\n",
       " '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@markdown ## Combine the search results into an answer using an LLM\n",
    "# Combine the LLM with a prompt to make a simple chain\n",
    "prompt = PromptTemplate(input_variables=['query', 'results'],\n",
    "                        template=\"\"\"Please summarize the following contextual data to answer the following question. Provide references to the context in your answer:\n",
    "Question: {query}\n",
    "Context:\n",
    "{results}\n",
    "Answer with citations:\"\"\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "\n",
    "summary = chain.run({\"query\": COMPLEX_QUERY, \"results\": results})\n",
    "\n",
    "summary.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNXkRWgVwjOE",
    "outputId": "45a1fee9-b827-4c3b-cd30-0c0a2422f442"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search 1: 1 documents\n",
      "Search 2: 1 documents\n"
     ]
    }
   ],
   "source": [
    "#@title We are using the same search engine, terms and question as the previous example\n",
    "\n",
    "\n",
    "## Here is the function being used to convert search results to documents\n",
    " ## (already encoded in the `EnterpriseSearchRetriever` class)\n",
    "\n",
    "# def search_response_to_documents(res) -> List[Document]:\n",
    "#     \"\"\"Retrieve langchain Documents from a search response\"\"\"\n",
    "#     documents = []\n",
    "#     for result in res.results:\n",
    "#         data = MessageToDict(result.document._pb)\n",
    "#         metadata = data.copy()\n",
    "#         del metadata['derivedStructData']\n",
    "#         del metadata['structData']\n",
    "#         if data.get('derivedStructData') is None:\n",
    "#             content = json.dumps(data.get('structData', {}))\n",
    "#         else:\n",
    "#             content = json.dumps([d.get('snippet') for d in data.get('derivedStructData', {}).get('snippets', []) if d.get('snippet') is not None])\n",
    "#         documents.append(Document(page_content=content, metadata=metadata))\n",
    "#     return documents\n",
    "\n",
    "#@markdown ## Search for each search term and extract into a langchain `Document` format\n",
    "#@markdown * This format just contains the snippets as `page_content` and the document title and link as `metadata`\n",
    "\n",
    "document_responses = []\n",
    "for t in clean_terms:\n",
    "    document_responses.append(retriever.get_relevant_documents(t))\n",
    "\n",
    "# This chain will run one LLM call for every document, so we likely do not want to keep all of the context if the document count is very large\n",
    "for idx, d in enumerate(document_responses):\n",
    "  print(f\"Search {idx + 1}: {len(d)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRCNBnq51SVd",
    "outputId": "19a54994-efc7-4403-d0e1-73f702ecb3bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 65 documents total, so we will just keep the top 3 from each search\n",
    "final_documents = [d for r in document_responses for d in r[:3]]\n",
    "\n",
    "len(final_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aGiLpkszfKD",
    "outputId": "1dc3e86d-a04f-491d-dced-a0ff66635d9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intermediate_steps': [' policy terms are the conditions and rules that govern an insurance policy',\n",
       "  ' Policy terms are the conditions and rules that govern an insurance policy. \\n\\nInsurance companies use policy terms to outline the coverage, exclusions, and limitations of their policies. Policy terms can vary significantly from one insurance company to another, so it is important to read and understand the policy terms before purchasing an insurance policy.'],\n",
       " 'output_text': ' Policy terms are the conditions and rules that govern an insurance policy. \\n\\nInsurance companies use policy terms to outline the coverage, exclusions, and limitations of their policies. Policy terms can vary significantly from one insurance company to another, so it is important to read and understand the policy terms before purchasing an insurance policy.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Load and run the chain using the documents\n",
    "#@markdown **Reminder**: the question is 'What are the voice and data packages available for roaming in Europe?'\n",
    "\n",
    "#@markdown Here we are printing the intermediate steps so you can see the end to end process\n",
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"refine\", return_refine_steps=True)\n",
    "\n",
    "chain({\"input_documents\": final_documents, \"question\": COMPLEX_QUERY}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bkb_ibQF-oC",
    "tags": []
   },
   "source": [
    "### Step 1 - Create tools\n",
    "In the **Imports and classes** section at the beginning of this notebook we defined a custom `EnterpriseSearchRetriever` class. This class exposes methods to retrieve search snippets and Documents. We pass these methods as 'tools' for the ReAct agent to use. Currently we have:\n",
    "\n",
    "* `get_relevant_snippets` and `get_relevant_documents`:\n",
    "  * Return a `List[str]` or `List[Document]` respectively of search results for a given search query\n",
    "\n",
    "We also define a function later to use an LLM to split a complex query into multiple search terms, as we did in the previous pattern\n",
    "* `extract_search_terms`:\n",
    "  * Return a `List[str]` of search keywords for a given complex query\n",
    "  * This uses the same process defined above to ask an LLM to split a complex query into simple keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "DlTNzi6lIWvP",
    "outputId": "ca657c14-93ac-481d-bd12-5fb482e4a33f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: Do I need to use a tool? No\n",
      "AI: Types of Insurance \n",
      "\n",
      "a. Life Insurance \n",
      "b. Health Insurance \n",
      "c. Motor Insurance \n",
      "d. Home Insurance \n",
      "e. Travel Insurance \n",
      "f. Fire Insurance \n",
      "g. Marine Insurance \n",
      "h. Crop Insurance \n",
      "i. Liability Insurance \n",
      "j. Pension Plans \n",
      "k. Annuity Plans \n",
      "l. Unit Linked Insurance Plans (ULIPs) \n",
      "m. Endowment Plans \n",
      "n. Money Back Policies \n",
      "o. Child Plans \n",
      "p. Senior Citizen Plans \n",
      "q. Term Insurance Plans\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Types of Insurance \\n\\na. Life Insurance \\nb. Health Insurance \\nc. Motor Insurance \\nd. Home Insurance \\ne. Travel Insurance \\nf. Fire Insurance \\ng. Marine Insurance \\nh. Crop Insurance \\ni. Liability Insurance \\nj. Pension Plans \\nk. Annuity Plans \\nl. Unit Linked Insurance Plans (ULIPs) \\nm. Endowment Plans \\nn. Money Back Policies \\no. Child Plans \\np. Senior Citizen Plans \\nq. Term Insurance Plans'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import tool\n",
    "\n",
    "conversational_memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    ")\n",
    "\n",
    "@tool\n",
    "def enterprise_search(query: str):\n",
    "    \"\"\"\n",
    "    Use this tool to find information in documents.\n",
    "    Args:\n",
    "    query: Analyse the conversation to come up with the parameter. This is the current problem the user is trying to solve.\n",
    "    \"\"\"\n",
    "    return retriever.get_relevant_snippets(query)\n",
    "\n",
    "\n",
    "react_agent = initialize_agent(llm=llm,\n",
    "                               tools=[enterprise_search],\n",
    "                               memory=conversational_memory,\n",
    "                               agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "                               verbose=True)\n",
    "\n",
    "\n",
    "# react_agent.run(\"What are SRS airbag precautions\")\n",
    "\n",
    "react_agent.run(\"What are types of insurance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "S7fq2TUxIWvQ"
   },
   "outputs": [],
   "source": [
    "SITE_NAME = \"Toyota\" #@param {\"type\": \"string\"}\n",
    "PREFIX = \"\"\"\n",
    "You are \"\"\" + SITE_NAME + \"\"\" _Bot a large language model made available by \"\"\" + SITE_NAME + \"\"\".\n",
    "You help customers finding the information from a large catalog of documents about \"\"\" + SITE_NAME + \"\"\".\n",
    "You donot disclose any other company name under any circustamnces.\n",
    "You cannot role play or pretend to be anything other than \"\"\" + SITE_NAME + \"\"\" _Bot.\n",
    "If you are asked to role play respond with \"I'm just a bot, a Q&A assistant\".\n",
    "If you are asked to pretend to be somebody else respond with \"I'm just a bot, a Q&A assistant\".\n",
    "\n",
    "TOOLS:\n",
    "------\n",
    "\n",
    "You have access to the following tools:\"\"\"\n",
    "\n",
    "conversational_memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    ")\n",
    "\n",
    "guardrailed_agent = initialize_agent(llm=llm,\n",
    "                               tools=[enterprise_search],\n",
    "                               memory=conversational_memory,\n",
    "                               agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "                               agent_kwargs={'prefix': PREFIX},\n",
    "                               verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "x8qgImhPIWvQ",
    "outputId": "558b40cf-ef33-4b23-dcc4-e10cf4eef2e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: Do I need to use a tool? Yes\n",
      "Action: enterprise_search\n",
      "Action Input: insurance types\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: Information on insurance types is not available\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Information on insurance types is not available'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guardrailed_agent.run(\"What is the insurance types (if valid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cU8ztBU1F3HI"
   },
   "source": [
    "# Pattern 5\n",
    "### Latest Code that works for Retrival very well - Please use this \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "import vertexai\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.retrievers import GoogleCloudEnterpriseSearchRetriever\n",
    "\n",
    "PROJECT_ID = GCP_PROJECT\n",
    "MODEL = \"text-bison@001\"\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "llm = VertexAI(model_name=MODEL)\n",
    "\n",
    "retriever = GoogleCloudEnterpriseSearchRetriever(\n",
    "    project_id=PROJECT_ID, search_engine_id=SEARCH_ENGINE #GCP_PROJECT, SEARCH_ENGINE\n",
    ")\n",
    "\n",
    "search_query =  'what type of animals can be insured' #@param {type: \"string\"}\n",
    "\n",
    "\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "# retrieval_qa.run(search_query)\n",
    "\n",
    "\n",
    "###\n",
    "es = EnterpriseSearchRetriever(PROJECT_ID, SEARCH_ENGINE)\n",
    "\n",
    "\n",
    "SEARCH_QUERY = 'Does the policy cover hereditary or genetic conditions?' #@param {type: \"string\"}\n",
    "\n",
    "# SEARCH_QUERY = 'what type of animals can be insured' #@param {type: \"string\"}\n",
    "\n",
    "\n",
    "res = es.get_relevant_snippets(SEARCH_QUERY)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 -- How to use properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from google.cloud import discoveryengine\n",
    "\n",
    "# TODO(developer): Uncomment these variables before running the sample.\n",
    "project_id = PROJECT_ID\n",
    "location = 'global'\n",
    "search_engine_id = SEARCH_ENGINE\n",
    "serving_config_id = \"default_config\"          # Values: \"default_config\"\n",
    "# search_query = SEARCH_QUERY\n",
    "\n",
    "\n",
    "def search_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    search_engine_id: str,\n",
    "    serving_config_id: str,\n",
    "    search_query: str,\n",
    ") -> List[discoveryengine.SearchResponse.SearchResult]:\n",
    "    # Create a client\n",
    "    client = discoveryengine.SearchServiceClient()\n",
    "\n",
    "    # The full resource name of the search engine serving config\n",
    "    # e.g. projects/{project_id}/locations/{location}\n",
    "    serving_config = client.serving_config_path(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        data_store=search_engine_id,\n",
    "        serving_config=serving_config_id,\n",
    "        # content_search_spec = {\"extractive_content_spec\": {\"max_extractive_answer_count\": 1}}\n",
    "        #{\"query\":\"what is google cloud\",\"content_search_spec\": {\"extractive_content_spec\": {\"max_extractive_answer_count\": 1}}}\n",
    "\n",
    "    )\n",
    "\n",
    "    request = discoveryengine.SearchRequest(\n",
    "        serving_config=serving_config, query=search_query, page_size=10\n",
    "    )\n",
    "    response = client.search(request)\n",
    "    for result in response.results:\n",
    "        print(result)\n",
    "\n",
    "    return response.results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_query = \"What is the Combination meter layout\"\n",
    "\n",
    "# search_query = \"how to change  temperature display in prius\"\n",
    "\n",
    "search_query = 'what type of animals can be insured' #@param {type: \"string\"}\n",
    "\n",
    "# projects/255766800726/locations/us-central1/collections/default_collection/dataStores/grab02-sm_1690440104277\n",
    "# projects/255766800726/locations/global/collections/default_collection/dataStores/grab02-sm_1690440104277/branches/0/documents/581eedaab781dc7ead70f28af4491766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: \"6808bc60247fbff372dc2d9745e5944a\"\n",
      "document {\n",
      "  name: \"projects/255766800726/locations/global/collections/default_collection/dataStores/igloo-store_1695195086499/branches/0/documents/6808bc60247fbff372dc2d9745e5944a\"\n",
      "  id: \"6808bc60247fbff372dc2d9745e5944a\"\n",
      "  derived_struct_data {\n",
      "    fields {\n",
      "      key: \"link\"\n",
      "      value {\n",
      "        string_value: \"gs://my-project-0004-bucket/matching_engine/Igloo_policy.txt\"\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"extractive_answers\"\n",
      "      value {\n",
      "        list_value {\n",
      "          values {\n",
      "            struct_value {\n",
      "              fields {\n",
      "                key: \"content\"\n",
      "                value {\n",
      "                  string_value: \"3. The object of insurance is a pet that is kept by the insured as a daily companion that must be cared for and fulfilled for their needs and have a proper place. 4. Pets in this policy are: 4.1. Dogs that are registered and have a Stambum 4.2. Cats that are registered and have a certificate from ICA 4.3.\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Final Answer 3. The object of insurance is a pet that is kept by the insured as a daily companion that must be cared for and fulfilled for their needs and have a proper place. 4. Pets in this policy are: 4.1. Dogs that are registered and have a Stambum 4.2. Cats that are registered and have a certificate from ICA 4.3.\n"
     ]
    }
   ],
   "source": [
    "search_output = search_sample(PROJECT_ID,location,SEARCH_ENGINE,serving_config_id,search_query)\n",
    "\n",
    "for document in search_output:\n",
    "    # print(document)\n",
    "    data = MessageToDict(document._pb)\n",
    "    # print(data)\n",
    "    data1 = data.get('document', {})\n",
    "    # print(data1)\n",
    "    data2 = data1.get('derivedStructData', {})\n",
    "    data3 = data2.get('extractive_answers', {})\n",
    "    data4 = data3[0]\n",
    "    query_result = data4.get('content', {})\n",
    "    \n",
    "    print(\"Final Answer\", query_result )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer pagenumber {}\n"
     ]
    }
   ],
   "source": [
    "# search_output = search_sample(PROJECT_ID,location,SEARCH_ENGINE,serving_config_id,search_query)\n",
    "\n",
    "for document in search_output:\n",
    "    # print(document)\n",
    "    data = MessageToDict(document._pb)\n",
    "    # print(data)\n",
    "    data1 = data.get('document', {})\n",
    "    # print(data1)\n",
    "    data2 = data1.get('derivedStructData', {})\n",
    "    data3 = data2.get('extractive_answers', {})\n",
    "    data4 = data3[0]\n",
    "    pagenumber_result = data4.get('pageNumber', {})\n",
    "    \n",
    "    print(\"Final Answer pagenumber\", pagenumber_result )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer 3. The object of insurance is a pet that is kept by the insured as a daily companion that must be cared for and fulfilled for their needs and have a proper place. 4. Pets in this policy are: 4.1. Dogs that are registered and have a Stambum 4.2. Cats that are registered and have a certificate from ICA 4.3.\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Answer\", query_result )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Less Instructive Answerrs and simple human readable summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is some information from database : 3. The object of insurance is a pet that is kept by the insured as a daily companion that must be cared for and fulfilled for their needs and have a proper place. 4. Pets in this policy are: 4.1. Dogs that are registered and have a Stambum 4.2. Cats that are registered and have a certificate from ICA 4.3. and here is the question user asked -- : what type of animals can be insured give a summary \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: Do I need to use a tool? No\n",
      "AI: Only dogs and cats can be insured. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Only dogs and cats can be insured.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_prompt = \"here is some information from database : \" + query_result + \" and here is the question user asked -- : \" + search_query + \" give a summary \"\n",
    "print(overall_prompt)\n",
    "\n",
    "react_agent.run(overall_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Instructive Answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_prompt = \"here is some information from database : \" + query_result + \" and here is the question user asked -- : \" + search_query + \" give a summary based on the inforamtion from database but keep as detailed as possible \"\n",
    "# print(overall_prompt)\n",
    "\n",
    "# react_agent.run(overall_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very high level summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: The object of insurance is a pet that is kept by the insured as a daily companion that must be cared for and fulfilled for their needs and have a proper place. Pets in this policy are: 4.1. Dogs that are registered and have a Stambum 4.2. Cats that are registered and have a certificate from ICA 4.3.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The object of insurance is a pet that is kept by the insured as a daily companion that must be cared for and fulfilled for their needs and have a proper place. Pets in this policy are: 4.1. Dogs that are registered and have a Stambum 4.2. Cats that are registered and have a certificate from ICA 4.3.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result = query_result #@param {\"type\": \"string\"}\n",
    "PREFIX = \"\"\"\n",
    "You are helpful agent able to answer any question only\n",
    "based on the information from information here \"\"\" + query_result \n",
    "\n",
    "conversational_memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    ")\n",
    "\n",
    "guardrailed_agent = initialize_agent(llm=llm,\n",
    "                               tools=[enterprise_search],\n",
    "                               memory=conversational_memory,\n",
    "                               agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "                               agent_kwargs={'prefix': PREFIX},\n",
    "                               verbose=True)\n",
    "\n",
    "guardrailed_agent.run(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is some information from database : 3. The object of insurance is a pet that is kept by the insured as a daily companion that must be cared for and fulfilled for their needs and have a proper place. 4. Pets in this policy are: 4.1. Dogs that are registered and have a Stambum 4.2. Cats that are registered and have a certificate from ICA 4.3. and here is the question user asked -- : what type of animals can be insured give a summary based on the inforamtion from database but keep as detailed as possible \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: Do I need to use a tool? No\n",
      "AI: Only dogs and cats can be insured, and they must be registered and have the appropriate documentation. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Only dogs and cats can be insured, and they must be registered and have the appropriate documentation.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query_result = \"What is the Combination meter , maintenance teams says it needs to be replaced \"\n",
    "\n",
    "overall_prompt = \"here is some information from database : \" + query_result + \" and here is the question user asked -- : \" + search_query + \" give a summary based on the inforamtion from database but keep as detailed as possible \"\n",
    "print(overall_prompt)\n",
    "\n",
    "react_agent.run(overall_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extra Function-- house keeping\n",
    "## Data Sore of ES - Querying direct \n",
    "!pip install google-cloud-discoveryengine --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-discoveryengine --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Store Index Status\n",
    "<!-- Using the list_documents method, we can do a check to see if the data store has finished indexing. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from google.cloud import discoveryengine_v1\n",
    "from google.api_core import operations_v1, grpc_helpers\n",
    "from google.longrunning import operations_pb2\n",
    "from typing import List, Optional\n",
    "\n",
    "def list_documents(\n",
    "    project_id: str, location: str, datastore_id: str, rate_limit: int = 1):\n",
    "  \"\"\"Gets a list of docs in a datastore.\"\"\"\n",
    "  client = discoveryengine_v1.DocumentServiceClient()\n",
    "\n",
    "  request = discoveryengine_v1.ListDocumentsRequest(\n",
    "      parent=f'projects/{project_id}/locations/{location}/collections/default_collection/dataStores/{datastore_id}/branches/0',\n",
    "      page_size=1000\n",
    "  )\n",
    "\n",
    "  res = client.list_documents(request=request)\n",
    "\n",
    "  # setup the list with the first batch of docs\n",
    "  docs = res.documents\n",
    "\n",
    "  while res.next_page_token:\n",
    "    # implement a rate_limit to prevent quota exhaustion\n",
    "    time.sleep(rate_limit)\n",
    "\n",
    "    request = discoveryengine_v1.ListDocumentsRequest(\n",
    "      parent=f'projects/{project_id}/locations/{location}/collections/default_collection/dataStores/{datastore_id}/branches/0',\n",
    "      page_size=1000,\n",
    "      page_token=res.next_page_token\n",
    "    )\n",
    "\n",
    "    res = client.list_documents(request=request)\n",
    "    docs.extend(res.documents)\n",
    "\n",
    "  return docs\n",
    "\n",
    "def list_indexed_urls(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    datastore_id: str,\n",
    "    docs: Optional[List[discoveryengine_v1.Document]] = None):\n",
    "  \"\"\"Get the list of docs in datastore, then parse to only urls.\"\"\"\n",
    "  if not docs:\n",
    "    docs = list_documents(project_id, location, datastore_id)\n",
    "  urls = [doc.content.uri for doc in docs]\n",
    "\n",
    "  return urls\n",
    "\n",
    "def search_url(urls: List[str], url: str):\n",
    "  \"\"\"Searches a url in a list of urls.\"\"\"\n",
    "  for item in urls:\n",
    "    if url in item:\n",
    "      print(item)\n",
    "\n",
    "def search_doc_id(\n",
    "    doc_id: str, docs: Optional[List[discoveryengine_v1.Document]] = None):\n",
    "  \"\"\"Searches a doc_id in a list of docs.\"\"\"\n",
    "  if not docs:\n",
    "    docs = list_documents(project_id, location, datastore_id)\n",
    "\n",
    "  doc_found = False\n",
    "  for doc in docs:\n",
    "    if doc.parent_document_id == document_id:\n",
    "      doc_found = True\n",
    "      print(doc)\n",
    "\n",
    "  if not doc_found:\n",
    "    print(f\"Document not found for provided Doc ID: `{doc_id}`\")\n",
    "\n",
    "\n",
    "def get_operations_status(operation_id: str):\n",
    "  \"\"\"Get the status of an import operation for Discovery Engine.\"\"\"\n",
    "  host = \"discoveryengine.googleapis.com\"\n",
    "  channel = grpc_helpers.create_channel(host)\n",
    "  client = operations_v1.OperationsClient(channel)\n",
    "\n",
    "  response = client.get_operation(operation_id)\n",
    "\n",
    "  return response\n",
    "\n",
    "PENDING_MESSAGE = \"\"\"\n",
    "No docs found.\\n\\nIt\\'s likely one of two issues: \\n  [1] Your data store is not finished indexing. \\n  [2] Your data store failed indexing.\\n\n",
    "If you just added your data store, it can take up to 4 hours before it will become available.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Success! 🎉\n",
      "\n",
      "  Your indexing is complete.\n",
      "\n",
      "  Your index contains 1 documents.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "datastore_id = SEARCH_ENGINE\n",
    "\n",
    "docs = list_documents(project_id, location, datastore_id)\n",
    "\n",
    "if len(docs) == 0:\n",
    "  print(PENDING_MESSAGE)\n",
    "else:\n",
    "  SUCCESS_MESSAGE = f\"\"\"\n",
    "  Success! 🎉\\n\n",
    "  Your indexing is complete.\\n\n",
    "  Your index contains {len(docs)} documents.\n",
    "  \"\"\"\n",
    "  print(SUCCESS_MESSAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import discoveryengine_v1\n",
    "from typing import List\n",
    "\n",
    "def list_documents(project_id: str, location: str, datastore_id: str):\n",
    "  \"\"\"Gets a list of docs in a datastore.\"\"\"\n",
    "  client = discoveryengine_v1.DocumentServiceClient()\n",
    "\n",
    "  request = discoveryengine_v1.ListDocumentsRequest(\n",
    "      parent=f'projects/{project_id}/locations/{location}/collections/default_collection/dataStores/{datastore_id}/branches/0'\n",
    "  )\n",
    "\n",
    "  res = client.list_documents(request=request)\n",
    "\n",
    "  return list(res)\n",
    "\n",
    "def list_indexed_urls(project_id: str, location: str, datastore_id: str):\n",
    "  \"\"\"Get the list of docs in datastore, then parse to only urls.\"\"\"\n",
    "  docs = list_documents(project_id, location, datastore_id)\n",
    "  urls = [doc.content.uri for doc in docs]\n",
    "\n",
    "  return urls\n",
    "\n",
    "def search_url(urls: List[str], url: str):\n",
    "  \"\"\"Searches a url in a list of urls.\"\"\"\n",
    "  for item in urls:\n",
    "    if url in item:\n",
    "      print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_id = 'pmarlow-ccai-dev'\n",
    "# location = 'global'\n",
    "# datastore_id = 'grab02-sm_1690440104277'\n",
    "\n",
    "# datastore_id = \"ntu-vide-transcript-combo-_1692590014861\"\n",
    "\n",
    "project_id = project=\"my-project-0004-346516\" \n",
    "location=  'global' #\"us-central1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Documents (in Data Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "struct_data {\n",
       "}\n",
       "name: \"projects/255766800726/locations/global/collections/default_collection/dataStores/igloo-store_1695195086499/branches/0/documents/6808bc60247fbff372dc2d9745e5944a\"\n",
       "id: \"6808bc60247fbff372dc2d9745e5944a\"\n",
       "schema_id: \"default_schema\"\n",
       "content {\n",
       "  uri: \"gs://my-project-0004-bucket/matching_engine/Igloo_policy.txt\"\n",
       "  mime_type: \"text/plain\"\n",
       "}\n",
       "parent_document_id: \"6808bc60247fbff372dc2d9745e5944a\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = list_documents(project_id, location, datastore_id)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Indexed URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://my-project-0004-bucket/matching_engine/Igloo_policy.txt'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = list_indexed_urls(project_id, location, datastore_id)\n",
    "urls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Search Indexed URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url(urls, 'https://www.yeti.com/bags/luggage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url(urls, 'toyota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8hOvJZRHiQ6z"
   ],
   "provenance": []
  },
  "environment": {
   "kernel": "py311",
   "name": "pytorch-gpu.1-12.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m98"
  },
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
